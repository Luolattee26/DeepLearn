{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f7b20a36-f16c-47bf-b24d-fd082fea1e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e21a63dd-3899-466e-a290-61942661fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义好参数的真实值\n",
    "true_w = torch.tensor([10.0, -5.0]) # 不加.0会变成int\n",
    "true_b = 10.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6947f729-def8-4fad-8636-79111c2ea678",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.normal(0 ,1, (1000, 2)) # 这个生成数据的方法真的方便\n",
    "y = torch.matmul(x, true_w) + true_b\n",
    "y = y + torch.normal(0, 1, y.shape) # 加上噪音"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "92f314a7-94db-434d-8f5d-01b33f12e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "68c45270-7c74-4f4d-9a5e-7c0bf2f91539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向框架里面读入数据\n",
    "dataset = data.TensorDataset(*(x, y)) # x, y一起输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bdaf7114-fb6d-4c65-a883-1cd876848b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置超参数\n",
    "batch_size = 20\n",
    "# 迭代对象\n",
    "data_iter = data.DataLoader(dataset, batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2fd8e171-f42a-43c0-9427-195800304ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.2312, -2.0429],\n",
       "         [ 0.6349,  1.2550],\n",
       "         [ 0.0605,  0.5054],\n",
       "         [ 1.0682, -1.5177],\n",
       "         [-0.4823, -0.1926],\n",
       "         [ 0.0352,  0.9522],\n",
       "         [ 1.3950,  0.5049],\n",
       "         [-0.9316,  1.1041],\n",
       "         [-0.9435,  0.5335],\n",
       "         [ 0.9577,  0.5793],\n",
       "         [-2.0006, -0.6170],\n",
       "         [-0.9044,  0.2357],\n",
       "         [ 0.2108, -0.1710],\n",
       "         [ 1.8029, -1.5121],\n",
       "         [ 0.2421, -1.1206],\n",
       "         [ 0.3719,  0.0709],\n",
       "         [ 0.0544, -1.9531],\n",
       "         [ 0.4688, -0.3470],\n",
       "         [ 0.4639, -0.4959],\n",
       "         [ 1.1702, -0.5648]]),\n",
       " tensor([22.3285,  9.8088,  7.9114, 28.6697,  5.8459,  7.2750, 21.6267, -4.1066,\n",
       "         -2.2411, 17.1890, -7.2277, -1.1012, 12.5431, 35.3295, 18.3010, 12.5077,\n",
       "         20.2271, 15.4461, 18.2069, 23.0314])]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_iter)) # 可以发现每次取20个，也就是我们设置好的batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9a3cf25c-91af-4f51-b371-bbd98ef7468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn # 神经网络模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4afafc13-82e0-415b-ae60-5981ffdf4200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 就是一个最简单的多层感知机（1层），实际上就是线性回归\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(2, 1) # input/out 的格式\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "77c2c611-33ec-488b-9b6f-52a0a65060c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 进行参数初始化\n",
    "# 这里可以发现，net对象就是我们所构建的网络\n",
    "# 0表示第一个网络，因为这里网络只有一层\n",
    "net[0].weight.data.normal_(0, 0.01)\n",
    "net[0].bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ce0207a3-0ef8-4a72-965b-56c0928bb480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数L\n",
    "# L2的loss torch都直接写好了\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9ccb9053-e6e5-4925-b12a-20a1059cfb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接下来就可以开始训练了\n",
    "# 用了最简单的直接梯度下降法\n",
    "# 要训练的对象，以及训练率\n",
    "trainer = torch.optim.SGD(net.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2637c057-7427-4465-925b-79e6725912d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100 # 设置训练的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "47a5eb8e-ee18-4b5d-a852-d5ffe4e9d963",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(137.6104, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.0476, -0.1523]]) tensor([6.0735])\n",
      "1 tensor(126.4426, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.3110, -0.1285]]) tensor([8.4376])\n",
      "2 tensor(125.2037, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.3422, -0.2340]]) tensor([9.2254])\n",
      "3 tensor(125.1052, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.3830, -0.3090]]) tensor([9.5181])\n",
      "4 tensor(125.0939, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4018, -0.2774]]) tensor([9.5565])\n",
      "5 tensor(125.0548, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4122, -0.1872]]) tensor([9.6968])\n",
      "6 tensor(125.3006, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.6044, -0.2940]]) tensor([9.6111])\n",
      "7 tensor(125.2074, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5534, -0.2272]]) tensor([9.6264])\n",
      "8 tensor(125.2048, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5055, -0.3024]]) tensor([9.7787])\n",
      "9 tensor(125.1676, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5046, -0.2262]]) tensor([9.7817])\n",
      "10 tensor(125.2051, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5015, -0.3313]]) tensor([9.6363])\n",
      "11 tensor(125.1520, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4633, -0.2808]]) tensor([9.7769])\n",
      "12 tensor(125.0824, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4501, -0.1743]]) tensor([9.6405])\n",
      "13 tensor(125.0657, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4263, -0.1534]]) tensor([9.5543])\n",
      "14 tensor(125.0935, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4685, -0.1425]]) tensor([9.6001])\n",
      "15 tensor(125.2112, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5328, -0.2701]]) tensor([9.5742])\n",
      "16 tensor(125.1481, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4443, -0.3039]]) tensor([9.5411])\n",
      "17 tensor(125.1448, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4897, -0.2377]]) tensor([9.6262])\n",
      "18 tensor(125.0012, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.3537, -0.1592]]) tensor([9.6123])\n",
      "19 tensor(125.0741, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4277, -0.2074]]) tensor([9.6756])\n",
      "20 tensor(125.0719, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4207, -0.2049]]) tensor([9.7296])\n",
      "21 tensor(125.0605, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4425, -0.1203]]) tensor([9.6646])\n",
      "22 tensor(125.0525, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4153, -0.1737]]) tensor([9.6269])\n",
      "23 tensor(125.2650, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5679, -0.2919]]) tensor([9.7747])\n",
      "24 tensor(125.2546, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5988, -0.2174]]) tensor([9.6730])\n",
      "25 tensor(125.1082, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4384, -0.2642]]) tensor([9.6805])\n",
      "26 tensor(125.1054, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4606, -0.2100]]) tensor([9.6959])\n",
      "27 tensor(125.2682, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5917, -0.2573]]) tensor([9.7356])\n",
      "28 tensor(125.1890, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5285, -0.2499]]) tensor([9.6509])\n",
      "29 tensor(125.2647, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5868, -0.2665]]) tensor([9.7180])\n",
      "30 tensor(125.2344, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5800, -0.2224]]) tensor([9.6733])\n",
      "31 tensor(125.1671, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5169, -0.2233]]) tensor([9.6178])\n",
      "32 tensor(125.0512, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.3909, -0.2256]]) tensor([9.6807])\n",
      "33 tensor(125.1354, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4164, -0.3469]]) tensor([9.6609])\n",
      "34 tensor(125.1707, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4666, -0.3183]]) tensor([9.5710])\n",
      "35 tensor(125.2701, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5823, -0.2891]]) tensor([9.7080])\n",
      "36 tensor(125.1910, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5146, -0.2755]]) tensor([9.7308])\n",
      "37 tensor(125.0665, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.3968, -0.2280]]) tensor([9.7589])\n",
      "38 tensor(125.0974, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4397, -0.2282]]) tensor([9.7290])\n",
      "39 tensor(125.2124, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5467, -0.2445]]) tensor([9.7451])\n",
      "40 tensor(125.2941, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5903, -0.3162]]) tensor([9.7008])\n",
      "41 tensor(125.1237, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5033, -0.1395]]) tensor([9.6888])\n",
      "42 tensor(125.2209, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5776, -0.1879]]) tensor([9.7152])\n",
      "43 tensor(125.3177, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.6362, -0.2505]]) tensor([9.6205])\n",
      "44 tensor(125.2911, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5629, -0.3640]]) tensor([9.6588])\n",
      "45 tensor(125.1333, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4748, -0.2439]]) tensor([9.6234])\n",
      "46 tensor(125.1699, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4819, -0.2223]]) tensor([9.4606])\n",
      "47 tensor(125.2723, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5623, -0.3208]]) tensor([9.5705])\n",
      "48 tensor(125.3331, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5930, -0.3741]]) tensor([9.6581])\n",
      "49 tensor(125.2611, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5252, -0.3756]]) tensor([9.7234])\n",
      "50 tensor(125.2709, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5794, -0.2993]]) tensor([9.6854])\n",
      "51 tensor(125.2724, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.6134, -0.2165]]) tensor([9.6850])\n",
      "52 tensor(125.2319, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5623, -0.2612]]) tensor([9.6920])\n",
      "53 tensor(125.1238, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4701, -0.2233]]) tensor([9.5884])\n",
      "54 tensor(125.2122, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5497, -0.2498]]) tensor([9.6452])\n",
      "55 tensor(125.2108, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5702, -0.1889]]) tensor([9.6894])\n",
      "56 tensor(125.2404, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5531, -0.2952]]) tensor([9.6193])\n",
      "57 tensor(125.1021, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.3925, -0.3237]]) tensor([9.6825])\n",
      "58 tensor(125.1216, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.3932, -0.3383]]) tensor([9.7611])\n",
      "59 tensor(125.2612, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.6133, -0.1875]]) tensor([9.6682])\n",
      "60 tensor(125.1676, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5275, -0.1992]]) tensor([9.6927])\n",
      "61 tensor(125.1772, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5234, -0.2296]]) tensor([9.6108])\n",
      "62 tensor(125.1327, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4986, -0.1875]]) tensor([9.6601])\n",
      "63 tensor(125.1121, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4498, -0.2228]]) tensor([9.7735])\n",
      "64 tensor(125.0689, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4185, -0.1426]]) tensor([9.8133])\n",
      "65 tensor(125.2197, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5025, -0.3447]]) tensor([9.7413])\n",
      "66 tensor(125.2173, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4493, -0.4150]]) tensor([9.7347])\n",
      "67 tensor(125.1712, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4840, -0.2852]]) tensor([9.7647])\n",
      "68 tensor(125.0813, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4397, -0.1895]]) tensor([9.7198])\n",
      "69 tensor(125.0600, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4101, -0.1754]]) tensor([9.7695])\n",
      "70 tensor(125.1021, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4651, -0.1892]]) tensor([9.6263])\n",
      "71 tensor(125.1242, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4524, -0.2712]]) tensor([9.6794])\n",
      "72 tensor(125.0967, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.3997, -0.2716]]) tensor([9.7924])\n",
      "73 tensor(125.2181, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5400, -0.2701]]) tensor([9.7499])\n",
      "74 tensor(125.2819, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5466, -0.3657]]) tensor([9.7540])\n",
      "75 tensor(125.1217, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4596, -0.2487]]) tensor([9.6156])\n",
      "76 tensor(125.2585, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5909, -0.1783]]) tensor([9.5008])\n",
      "77 tensor(125.1324, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4846, -0.2222]]) tensor([9.6834])\n",
      "78 tensor(125.1790, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5224, -0.2045]]) tensor([9.5388])\n",
      "79 tensor(125.1627, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5208, -0.2067]]) tensor([9.6503])\n",
      "80 tensor(125.2216, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5738, -0.2068]]) tensor([9.6407])\n",
      "81 tensor(125.1619, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5013, -0.2526]]) tensor([9.6562])\n",
      "82 tensor(125.1059, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4199, -0.2887]]) tensor([9.6198])\n",
      "83 tensor(125.4296, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.7156, -0.2492]]) tensor([9.5905])\n",
      "84 tensor(125.5278, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.7700, -0.2670]]) tensor([9.5526])\n",
      "85 tensor(125.1761, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5180, -0.2444]]) tensor([9.6945])\n",
      "86 tensor(125.0633, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4189, -0.1930]]) tensor([9.7078])\n",
      "87 tensor(125.0435, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4004, -0.1846]]) tensor([9.6338])\n",
      "88 tensor(125.1818, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4862, -0.3176]]) tensor([9.6920])\n",
      "89 tensor(125.1604, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4536, -0.3130]]) tensor([9.5473])\n",
      "90 tensor(125.2153, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4797, -0.3623]]) tensor([9.5500])\n",
      "91 tensor(125.1272, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4554, -0.2400]]) tensor([9.5372])\n",
      "92 tensor(125.1030, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4529, -0.2000]]) tensor([9.5624])\n",
      "93 tensor(125.1567, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4976, -0.1945]]) tensor([9.8127])\n",
      "94 tensor(125.1342, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4819, -0.2336]]) tensor([9.6730])\n",
      "95 tensor(125.1631, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.5406, -0.1287]]) tensor([9.7237])\n",
      "96 tensor(125.1357, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4997, -0.1914]]) tensor([9.6864])\n",
      "97 tensor(125.0866, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4194, -0.2536]]) tensor([9.6762])\n",
      "98 tensor(125.1954, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.4956, -0.2993]]) tensor([9.5381])\n",
      "99 tensor(125.0462, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.3915, -0.1992]]) tensor([9.5845])\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    for run_x, run_y in data_iter:\n",
    "        l = loss(net(run_x), run_y) # pytorch 的语法很接近原生Python\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    l = loss(net(x), y)\n",
    "    print(i, l)\n",
    "    print(net[0].weight.data, net[0].bias.data)\n",
    "\n",
    "# 可以发现这个训练例子是依托答辩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "905f7c49-37a7-4b3d-a7e1-dc85c11cf571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果是用MLP来构造一个minst的手写数字识别例子\n",
    "# 其实就是搭积木\n",
    "net = nn.Sequential(\n",
    "    nn.Flatten(),  # 首先把输入的矩阵转化为向量，方便后面的感知机隐藏层计算\n",
    "    nn.Linear(784, 127), \n",
    "    nn.ReLU(), # 线性模型用在分类问题中，就是用激活函数来转换，sigmoid函数也是一样的，不过relu最方便\n",
    "    nn.Linear(127, 10) # 这中间的隐藏层数、节点数是自己随便设置的，所以才会说是个黑箱\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f63f16-7efd-48ae-a94f-e09b95d85e15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
