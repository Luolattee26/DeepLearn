{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这玩意现在被用到的很少了，但是提出了一些现在也常用的概念，所以在这节讲一下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T18:56:11.932234Z",
     "iopub.status.busy": "2023-12-14T18:56:11.931328Z",
     "iopub.status.idle": "2023-12-14T18:56:11.954222Z",
     "shell.execute_reply": "2023-12-14T18:56:11.951898Z",
     "shell.execute_reply.started": "2023-12-14T18:56:11.932157Z"
    },
    "tags": []
   },
   "source": [
    "前面我们学习的LeNet、AlexNet、VGG在最后都是用的dense layer来结尾的，这会带来几个问题：\n",
    "1. 全连接层好多好多参数，会占用大量内存和计算带宽\n",
    "2. 全连接层本身也容易过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NiN的思想就是，哥们不用全连接层了，引入了特殊的结构NiN块（从VGG学过来的......）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NiN block其实就是，在conv layer后面加上两个11的卷积核，padding也是1，从而对通道进行一定的融合（或者扩充）\n",
    "\n",
    "可以发现，这玩意其实实际上就是全连接层，只不过套了个卷积层的皮套\n",
    "\n",
    "因为我们知道，通常不同通道用的卷积核是一样的，因为这样计算方便\n",
    "\n",
    "所以这里其实可以看做，NiN block对于每个像素做了一个全连接层，全连接层连了啥呢？也就是这个像素的不同通道，因为我通道间的卷积核是一样的，所以这样参数就会下来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NiN的整体架构可以总结为：\n",
    "1. 无全连接层\n",
    "2. 交替使用NiN block和stride 2的max pool\n",
    "3. 最后用全局平均池化层直接输出（加个softmax，写在training函数里面了），输出的通道数就是最终的预测类别数\n",
    "4. 全局平均池化层其实就是池化层，只不过是池化层之后直接进softmax，所以叫这个（之前的你池化层之后还是要过一下dense layer）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NiN架构的总结：\n",
    "1. 对每个像素用2个11卷积核，从而相比用全连接层增加了非线性\n",
    "2. 最后的池化层是全局池化层，整个覆盖，一个channel里面能学到的平均东西就是你的类别标识\n",
    "3. 全局池化层有效的降低了模型的复杂度，在后面的模型里面被广泛使用，不过这玩意收敛就慢很多了\n",
    "3. 因为完全没用全连接层，所以参数少调参方便，并不容易过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T19:13:28.971308Z",
     "iopub.status.busy": "2023-12-14T19:13:28.970494Z",
     "iopub.status.idle": "2023-12-14T19:13:28.978278Z",
     "shell.execute_reply": "2023-12-14T19:13:28.977178Z",
     "shell.execute_reply.started": "2023-12-14T19:13:28.971263Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "def nin_block(in_channels, out_channels, kernel_size, strides, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU()) # 这里连着两个11卷积核，其实通道数一直都没变化\n",
    "# 用两个11卷积层，这玩意应该是试出来的......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T19:13:58.651924Z",
     "iopub.status.busy": "2023-12-14T19:13:58.651466Z",
     "iopub.status.idle": "2023-12-14T19:13:58.685508Z",
     "shell.execute_reply": "2023-12-14T19:13:58.684449Z",
     "shell.execute_reply.started": "2023-12-14T19:13:58.651881Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nin_block(1, 96, kernel_size=11, strides=4, padding=0),\n",
    "    nn.MaxPool2d(3, stride=2),\n",
    "    nin_block(96, 256, kernel_size=5, strides=1, padding=2),\n",
    "    nn.MaxPool2d(3, stride=2),\n",
    "    nin_block(256, 384, kernel_size=3, strides=1, padding=1),\n",
    "    nn.MaxPool2d(3, stride=2),\n",
    "    nn.Dropout(0.5),\n",
    "    # 标签类别数是10\n",
    "    nin_block(384, 10, kernel_size=3, strides=1, padding=1),\n",
    "    nn.AdaptiveAvgPool2d((1, 1)), # 这玩意是全局平均池化\n",
    "    # 将四维的输出转成二维的输出，其形状为(批量大小,10)\n",
    "    nn.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T19:14:55.829535Z",
     "iopub.status.busy": "2023-12-14T19:14:55.828804Z",
     "iopub.status.idle": "2023-12-14T19:14:55.879542Z",
     "shell.execute_reply": "2023-12-14T19:14:55.878397Z",
     "shell.execute_reply.started": "2023-12-14T19:14:55.829476Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 96, 54, 54])\n",
      "MaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\n",
      "Sequential output shape:\t torch.Size([1, 256, 26, 26])\n",
      "MaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\n",
      "Sequential output shape:\t torch.Size([1, 384, 12, 12])\n",
      "MaxPool2d output shape:\t torch.Size([1, 384, 5, 5])\n",
      "Dropout output shape:\t torch.Size([1, 384, 5, 5])\n",
      "Sequential output shape:\t torch.Size([1, 10, 5, 5])\n",
      "AdaptiveAvgPool2d output shape:\t torch.Size([1, 10, 1, 1])\n",
      "Flatten output shape:\t torch.Size([1, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 1, 224, 224))\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 0.1, 10, 128\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\n",
    "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
