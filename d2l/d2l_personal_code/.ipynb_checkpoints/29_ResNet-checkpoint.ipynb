{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4943ad10-3f24-439f-b414-b70ea54808a9",
   "metadata": {},
   "source": [
    "真神降临"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5923a88-dec1-4c33-a535-a0cc457f9122",
   "metadata": {},
   "source": [
    "ResNet可以说是目前最为简单、但是同时非常实用的一个网络\n",
    "1. ResNet整体的思想其实就是，加深模型是可以的，但是往残差的方向加（boosting：好像很眼熟）\n",
    "2. 原来我们网络在不断更新的时候，其实是这样的f(x)=g(x)，f(x)是我们最终要拟合的函数，g就是我们不断来加入新的block让我们的网络变得更大\n",
    "3. ResNet的思想就是，我现在变成f(x)=g(x)+x，x是我们上一个block的输出，也就是说，哪怕我在网络上加新的block，这个新的玩意屁用没有，我至少有上一层的结果让我有个保底\n",
    "4. 可以发现这里和GoogleNet有点像，用了并行的结构，这种结构显然Sequential类没办法处理，这也就是为啥我们要自定义网络（回顾前面的）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab681a3-204f-4a9c-8098-1278c93e8404",
   "metadata": {},
   "source": [
    "仔细看RseNet block的结构，会发现这玩意和VGG挺像的，所以VGG的伟大无需多言\n",
    "* 不过这个block有各种各样的结构，都可以试试......反正各有各的说法，总体思想就是f(x)=g(x)+x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89652e20-cd12-4dcb-aa5e-76345eda02cb",
   "metadata": {},
   "source": [
    "整个ResNet的结构其实就是\n",
    "1. 先一个高宽减半的ResNet block，stride 2\n",
    "2. 然后多个高宽不变的ResNet block\n",
    "3. 然后排列组合\n",
    "4. 和Google Net以及VGG差不多"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca11cc-12e1-47dd-9016-8ad25aead537",
   "metadata": {},
   "source": [
    "为啥说ResNet牛逼呢，因为现在比他好的那些CNN，基本上很多都是他的变种，然后就算不是变种，也用了f(x)=g(x)+x这个思想\n",
    "* 而且ResNet一些大尺寸的网络，比如101、152啥的，本身表现也处于第一梯队\n",
    "* 而且f(x)=g(x)+x这个思想，让更大的网络能包含住小的网络，所以能训练出很深的网络，甚至1000层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d046d483-9a5d-471c-bd7d-38522e2111de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T15:53:22.757408Z",
     "iopub.status.busy": "2023-12-15T15:53:22.756461Z",
     "iopub.status.idle": "2023-12-15T15:53:26.817238Z",
     "shell.execute_reply": "2023-12-15T15:53:26.816239Z",
     "shell.execute_reply.started": "2023-12-15T15:53:22.757340Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "class Residual(nn.Module):  #@save\n",
    "    def __init__(self, input_channels, num_channels,\n",
    "                 use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels,\n",
    "                               kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
    "                               kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
    "                                   kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X) # 这玩意就是改通道的\n",
    "        Y += X\n",
    "        return F.relu(Y) # 还记得前面说的吗，relu反正不学习参数，所以直接用F的就好了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e4892f5-4b75-4f91-8732-ef68af68b829",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T15:53:48.748327Z",
     "iopub.status.busy": "2023-12-15T15:53:48.747042Z",
     "iopub.status.idle": "2023-12-15T15:53:48.775855Z",
     "shell.execute_reply": "2023-12-15T15:53:48.774238Z",
     "shell.execute_reply.started": "2023-12-15T15:53:48.748258Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 6, 6])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = Residual(3,3)\n",
    "X = torch.rand(4, 3, 6, 6)\n",
    "Y = blk(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83088007-fe90-42b8-a6f1-420866e0613f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T15:53:58.544478Z",
     "iopub.status.busy": "2023-12-15T15:53:58.544064Z",
     "iopub.status.idle": "2023-12-15T15:53:58.556480Z",
     "shell.execute_reply": "2023-12-15T15:53:58.555546Z",
     "shell.execute_reply.started": "2023-12-15T15:53:58.544444Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 3, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = Residual(3,6, use_1x1conv=True, strides=2)\n",
    "blk(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28297a1e-6253-4c7a-9099-7590402ae5e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T15:54:21.281936Z",
     "iopub.status.busy": "2023-12-15T15:54:21.281514Z",
     "iopub.status.idle": "2023-12-15T15:54:21.289470Z",
     "shell.execute_reply": "2023-12-15T15:54:21.288318Z",
     "shell.execute_reply.started": "2023-12-15T15:54:21.281895Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ef7d29e-d8ce-40c9-b800-f1e8c932f3bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T16:00:01.791486Z",
     "iopub.status.busy": "2023-12-15T16:00:01.790824Z",
     "iopub.status.idle": "2023-12-15T16:00:01.813169Z",
     "shell.execute_reply": "2023-12-15T16:00:01.811541Z",
     "shell.execute_reply.started": "2023-12-15T16:00:01.791429Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 这等于是一个stage，把一个个residual block的基本元素组合在一起\n",
    "def resnet_block(input_channels, num_channels, num_residuals,\n",
    "                 first_block=False):\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(Residual(input_channels, num_channels,\n",
    "                                use_1x1conv=True, strides=2))\n",
    "        else:\n",
    "            blk.append(Residual(num_channels, num_channels))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a418e262-3d11-4929-b9a8-1effa35ada7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T15:55:31.311431Z",
     "iopub.status.busy": "2023-12-15T15:55:31.310748Z",
     "iopub.status.idle": "2023-12-15T15:55:31.456337Z",
     "shell.execute_reply": "2023-12-15T15:55:31.455289Z",
     "shell.execute_reply.started": "2023-12-15T15:55:31.311362Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n",
    "# 用Sequential把多个block串起来，作为一个stage\n",
    "b3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
    "b4 = nn.Sequential(*resnet_block(128, 256, 2))\n",
    "b5 = nn.Sequential(*resnet_block(256, 512, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1123e470-6dbe-4b6d-bb7b-658b83042dac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T15:55:52.667339Z",
     "iopub.status.busy": "2023-12-15T15:55:52.666817Z",
     "iopub.status.idle": "2023-12-15T15:55:52.672941Z",
     "shell.execute_reply": "2023-12-15T15:55:52.672075Z",
     "shell.execute_reply.started": "2023-12-15T15:55:52.667306Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(b1, b2, b3, b4, b5,\n",
    "                    nn.AdaptiveAvgPool2d((1,1)),\n",
    "                    nn.Flatten(), nn.Linear(512, 10)) # 这里和GoogleNet基本上是一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac3102e8-bcaf-4f62-9bf2-3f4f69348683",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T15:56:00.939444Z",
     "iopub.status.busy": "2023-12-15T15:56:00.938972Z",
     "iopub.status.idle": "2023-12-15T15:56:00.990246Z",
     "shell.execute_reply": "2023-12-15T15:56:00.989196Z",
     "shell.execute_reply.started": "2023-12-15T15:56:00.939410Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 64, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 128, 28, 28])\n",
      "Sequential output shape:\t torch.Size([1, 256, 14, 14])\n",
      "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n",
      "AdaptiveAvgPool2d output shape:\t torch.Size([1, 512, 1, 1])\n",
      "Flatten output shape:\t torch.Size([1, 512])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 1, 224, 224))\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfad3c7-b5fe-48ad-b68b-3f4542ee61e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 0.05, 10, 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\n",
    "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ae3dd7-4a37-4ab0-8751-925296ec99b8",
   "metadata": {},
   "source": [
    "之前讲避免梯度消失的时候，有一个做法其实就是乘法变成加法，这玩意很大程度上就是ResNet的思想\n",
    "\n",
    "具体的数学上的简洁解释，可以参考李沐老师的视频"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef67179b-a5e3-4803-83e7-aae46682752b",
   "metadata": {},
   "source": [
    "这里有一个比较重要的核心前提就是：\n",
    "* 我们知道梯度是损失函数对参数进行求导，当网络中比较深的层，从神经网络的学习层面上来说这些比较深的层已经学到了很多特征，也就是这些层能够比较好的拟合到我们真实的数据（也就是函数）\n",
    "* 所以这些比较深的层，训练着训练着梯度就会变得很小（大概率），因为你的拟合能力比较好，所以损失小，自然梯度算出来就小\n",
    "* 那这样累乘下来，浅的层反向传播回来就会得到一个特别小的梯度，参数完全就更新不动了\n",
    "* 这时候如果用一个很大的学习率，那那些深层的网络的参数就会乱窜，这样肯定也是不对的\n",
    "* 前面我们说了bn层，为啥bn层能让我们设置一个比较大的lr呢？\n",
    "* 其实直观的理解非常好理解，我们上面提到了，深层网络的拟合能力比较好，因为已经学习到了很好的特征了\n",
    "* 从全连接层开始我们就知道，神经网络实际上不断干的就是学习特征，数据刚进来或者是说在底层网络的时候，可能是比较脏的，有很多噪音或者说很多弱特征\n",
    "* 那我们不断学不断学，神经网络不断学习、生成新的特征，最终做到很好的分类，这时候数据就变得很干净了\n",
    "* bn层干的事情就是，你不断学特征的过程，你的样本的分布肯定发生变化（因为你不断学习、生成特征，你的特征发生了改变），那我就尽量让你的样本分布比较稳定\n",
    "* 这样其实就是目前流行的一种解释，向数据里面人为的添加了噪音，你学习可以学习，但是我人为给你加一点噪音，让数据不要干净的那么快\n",
    "* 这样的话，我整个网络不同层学的东西会接近，那我深层浅层的拟合能力（也就对应了梯度）不会存在那么大的鸿沟，所以我可以适当的加大一点学习率，这样的话不就能加快收敛了吗"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
