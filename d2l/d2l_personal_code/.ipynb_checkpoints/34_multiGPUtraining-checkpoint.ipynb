{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f38a91fe-331e-45cd-a1e8-8b12b2f4236b",
   "metadata": {},
   "source": [
    "这节课主要是讲怎么进行多GPU的数据并行训练，用LeNet做示范\n",
    "\n",
    "这一节没办法在colab上弄，因为colab免费只给一个t4 gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb7cce80-3321-42b1-b2ea-bfbb8c5eeadf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T16:15:22.236906Z",
     "iopub.status.busy": "2023-12-16T16:15:22.235927Z",
     "iopub.status.idle": "2023-12-16T16:15:26.537362Z",
     "shell.execute_reply": "2023-12-16T16:15:26.536283Z",
     "shell.execute_reply.started": "2023-12-16T16:15:22.236837Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d63b270d-5282-462b-9853-2179d86f82ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T15:50:38.492546Z",
     "iopub.status.busy": "2023-12-16T15:50:38.491705Z",
     "iopub.status.idle": "2023-12-16T15:50:38.521025Z",
     "shell.execute_reply": "2023-12-16T15:50:38.519651Z",
     "shell.execute_reply.started": "2023-12-16T15:50:38.492476Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 初始化模型参数\n",
    "scale = 0.01\n",
    "W1 = torch.randn(size=(20, 1, 3, 3)) * scale\n",
    "b1 = torch.zeros(20)\n",
    "W2 = torch.randn(size=(50, 20, 5, 5)) * scale\n",
    "b2 = torch.zeros(50)\n",
    "W3 = torch.randn(size=(800, 128)) * scale\n",
    "b3 = torch.zeros(128)\n",
    "W4 = torch.randn(size=(128, 10)) * scale\n",
    "b4 = torch.zeros(10)\n",
    "params = [W1, b1, W2, b2, W3, b3, W4, b4]\n",
    "\n",
    "# 定义模型\n",
    "def lenet(X, params):\n",
    "    h1_conv = F.conv2d(input=X, weight=params[0], bias=params[1])\n",
    "    h1_activation = F.relu(h1_conv)\n",
    "    h1 = F.avg_pool2d(input=h1_activation, kernel_size=(2, 2), stride=(2, 2))\n",
    "    h2_conv = F.conv2d(input=h1, weight=params[2], bias=params[3])\n",
    "    h2_activation = F.relu(h2_conv)\n",
    "    h2 = F.avg_pool2d(input=h2_activation, kernel_size=(2, 2), stride=(2, 2))\n",
    "    h2 = h2.reshape(h2.shape[0], -1)\n",
    "    h3_linear = torch.mm(h2, params[4]) + params[5]\n",
    "    h3 = F.relu(h3_linear)\n",
    "    y_hat = torch.mm(h3, params[6]) + params[7]\n",
    "    return y_hat\n",
    "\n",
    "# 交叉熵损失函数\n",
    "loss = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43cc41f-1fc3-4d00-9806-bea523489574",
   "metadata": {},
   "source": [
    "上面这里我们用了F来定义各种层，还记得我们之前有提到过，F里面是不会自动更新参数的，所以我们这里树洞给他指定了参数，也就是传入了weight参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2550620e-1b09-485d-82ab-d933ef3ee9fd",
   "metadata": {},
   "source": [
    "在这里我们再阐述一遍两者的差别\n",
    "\n",
    "从功能来说两者相当，基于nn.Mudle能实现的层，使用nn.funtional也可实现，反之亦然，而且性能方面两者也没有太大差异。不过在具体使用时，两者还是有区别，主要区别如下：\n",
    "1. nn.Xxx继承于nn.Module，nn.Xxx 需要先实例化并传入参数，然后以函数调用的方式调用实例化的对象并传入输入数据。它能够很好的与nn.Sequential结合使用，而nn.functional.xxx无法与nn.Sequential结合使用。\n",
    "2. nn.Xxx不需要自己定义和管理weight、bias参数；而nn.functional.xxx需要你自己定义weight、bias，每次调用的时候都需要手动传入weight、bias等参数, 不利于代码复用。\n",
    "3. dropout操作在训练和测试阶段是有区别的，使用nn.Xxx方式定义dropout，在调用model.eval()之后，自动实现状态的转换，而使用nn.functional.xxx却无此功能。\n",
    "* 总的来说，两种功能都是相同的，但PyTorch官方推荐：具有学习参数的（例如，conv2d, linear, batch_norm)采用nn.Xxx方式。没有学习参数的（例如，maxpool, loss func, activation func）等根据个人选择使用nn.functional.xxx或者nn.Xxx方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e83bf574-7a31-4345-a977-0d61c0dd0832",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T15:50:40.140075Z",
     "iopub.status.busy": "2023-12-16T15:50:40.139507Z",
     "iopub.status.idle": "2023-12-16T15:50:40.149403Z",
     "shell.execute_reply": "2023-12-16T15:50:40.148184Z",
     "shell.execute_reply.started": "2023-12-16T15:50:40.140025Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1 权重: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "b1 梯度: None\n"
     ]
    }
   ],
   "source": [
    "# 因为我们的数据在不进行任何操作的情况下，肯定是默认放在主内存（或者是CPU）里面的，所以这里我们就用一个函数，把我们所有的参数放到GPU里面，并且记得设置需要梯度来更新\n",
    "def get_params(params, device):\n",
    "    new_params = [p.to(device) for p in params]\n",
    "    for p in new_params:\n",
    "        p.requires_grad_()\n",
    "    return new_params\n",
    "\n",
    "new_params = get_params(params, d2l.try_gpu(0))\n",
    "print('b1 权重:', new_params[1])\n",
    "print('b1 梯度:', new_params[1].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9d1ba5-3309-4ff2-a925-6a2189e7c1e2",
   "metadata": {},
   "source": [
    "这里的allreduce操作在HPC领域是一个很常见的操作，用人话说就是我把所有设备上的数据合并起来，然后计算到一个结果再交给所有设备（因为我们是对一个batch的数据做了并行还记得吗，等于是一个batch的数据分成了好几份）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba219b04-ce36-4870-81cf-4a91e631974c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T15:55:44.678283Z",
     "iopub.status.busy": "2023-12-16T15:55:44.677460Z",
     "iopub.status.idle": "2023-12-16T15:55:44.697960Z",
     "shell.execute_reply": "2023-12-16T15:55:44.696580Z",
     "shell.execute_reply.started": "2023-12-16T15:55:44.678197Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allreduce之前：\n",
      " tensor([[1., 1.]]) \n",
      " tensor([[2., 2.]])\n",
      "allreduce之后：\n",
      " tensor([[3., 3.]]) \n",
      " tensor([[3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "def allreduce(data):\n",
    "    # 这里写的是把所有GPU上的数据全部放到GPU0上面，然后GPU0上面算完了再放回去\n",
    "    # 注意两个for loop的计算符号不一样，其实就是一个合并的过程\n",
    "    # 还记得吗，我们做损失函数的时候是把所有样本的损失合并求均值来算的\n",
    "    # 这里allreduce不就是合并的操作吗\n",
    "    for i in range(1, len(data)):\n",
    "        data[0][:] += data[i].to(data[0].device)\n",
    "    for i in range(1, len(data)):\n",
    "        data[i][:] = data[0].to(data[i].device)\n",
    "        \n",
    "data = [torch.ones((1, 2), device=d2l.try_gpu(i)) * (i + 1) for i in range(2)]\n",
    "print('allreduce之前：\\n', data[0], '\\n', data[1])\n",
    "allreduce(data)\n",
    "print('allreduce之后：\\n', data[0], '\\n', data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb588618-95dd-401e-a4ed-12ccff2a8e59",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-12-16T15:59:44.441554Z",
     "iopub.status.busy": "2023-12-16T15:59:44.440745Z",
     "iopub.status.idle": "2023-12-16T15:59:44.579578Z",
     "shell.execute_reply": "2023-12-16T15:59:44.577465Z",
     "shell.execute_reply.started": "2023-12-16T15:59:44.441485Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m20\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      2\u001b[0m devices \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m), torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:1\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m----> 3\u001b[0m split \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput :\u001b[39m\u001b[38;5;124m'\u001b[39m, data)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload into\u001b[39m\u001b[38;5;124m'\u001b[39m, devices)\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py:44\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(inputs, target_gpus, dim)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# After scatter_map is called, a scatter_map cell will exist. This cell\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# has a reference to the actual function scatter_map, which has references\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# to a closure that has a reference to the scatter_map cell (because the\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# fn is recursive). To avoid this reference cycle, we set the function to\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# None, clearing the cell\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mscatter_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     scatter_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py:27\u001b[0m, in \u001b[0;36mscatter.<locals>.scatter_map\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatter_map\u001b[39m(obj):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mScatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_namedtuple(obj):\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mtype\u001b[39m(obj)(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(scatter_map, obj))]\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:96\u001b[0m, in \u001b[0;36mScatter.forward\u001b[0;34m(ctx, target_gpus, chunk_sizes, dim, input)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;129;01mand\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39minput_device \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# Perform CPU to GPU copies in a background stream\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     streams \u001b[38;5;241m=\u001b[39m [_get_stream(device) \u001b[38;5;28;01mfor\u001b[39;00m device \u001b[38;5;129;01min\u001b[39;00m target_gpus]\n\u001b[0;32m---> 96\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcomm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Synchronize with the copy stream\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/torch/nn/parallel/comm.py:189\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(tensor, devices, chunk_sizes, dim, streams, out)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     devices \u001b[38;5;241m=\u001b[39m [_get_device_index(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m devices]\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreams\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m devices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "# 这里其实很简单，就是把你的batch分开 分到GPU设备上\n",
    "data = torch.arange(20).reshape(4, 5)\n",
    "devices = [torch.device('cuda:0'), torch.device('cuda:1')]\n",
    "split = nn.parallel.scatter(data, devices) # 这个函数只支持cuda设备，所以到这里就运行不了了\n",
    "print('input :', data)\n",
    "print('load into', devices)\n",
    "print('output:', split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "820ceb22-5261-4dc4-933f-4d06394913ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T16:01:20.657158Z",
     "iopub.status.busy": "2023-12-16T16:01:20.656666Z",
     "iopub.status.idle": "2023-12-16T16:01:20.671737Z",
     "shell.execute_reply": "2023-12-16T16:01:20.670541Z",
     "shell.execute_reply.started": "2023-12-16T16:01:20.657122Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def split_batch(X, y, devices):\n",
    "    \"\"\"将X和y拆分到多个设备上\"\"\"\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    return (nn.parallel.scatter(X, devices),\n",
    "            nn.parallel.scatter(y, devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "667ca76b-370f-4b26-b97f-d45433244c1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T16:34:50.071534Z",
     "iopub.status.busy": "2023-12-16T16:34:50.070687Z",
     "iopub.status.idle": "2023-12-16T16:34:50.088574Z",
     "shell.execute_reply": "2023-12-16T16:34:50.086618Z",
     "shell.execute_reply.started": "2023-12-16T16:34:50.071462Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_batch(X, y, device_params, devices, lr):\n",
    "    X_shards, y_shards = split_batch(X, y, devices)\n",
    "    # 在每个GPU上分别计算损失\n",
    "    ls = [loss(lenet(X_shard, device_W), y_shard).sum()\n",
    "          for X_shard, y_shard, device_W in zip(\n",
    "              X_shards, y_shards, device_params)]\n",
    "    for l in ls:  # 反向传播在每个GPU上分别执行\n",
    "        l.backward()\n",
    "    # 将每个GPU的所有梯度相加，并将其广播到所有GPU\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(device_params[0])):\n",
    "            allreduce(\n",
    "                [device_params[c][i].grad for c in range(len(devices))])\n",
    "    # 在每个GPU上分别更新模型参数\n",
    "    for param in device_params:\n",
    "        d2l.sgd(param, lr, X.shape[0]) # 在这里，我们使用全尺寸的小批量\n",
    "        # 不过这里其实有点重复计算，其实可以一个GPU算完，然后更新过去就行\n",
    "        \n",
    "# import inspect\n",
    "# print(inspect.getsource(d2l.sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbd8e5d9-cf86-452b-9b49-313a35f70a7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T16:06:53.651602Z",
     "iopub.status.busy": "2023-12-16T16:06:53.651157Z",
     "iopub.status.idle": "2023-12-16T16:06:53.660808Z",
     "shell.execute_reply": "2023-12-16T16:06:53.660012Z",
     "shell.execute_reply.started": "2023-12-16T16:06:53.651568Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(num_gpus, batch_size, lr):\n",
    "    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "    devices = [d2l.try_gpu(i) for i in range(num_gpus)]\n",
    "    # 将模型参数复制到num_gpus个GPU\n",
    "    device_params = [get_params(params, d) for d in devices]\n",
    "    num_epochs = 10\n",
    "    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n",
    "    timer = d2l.Timer()\n",
    "    for epoch in range(num_epochs):\n",
    "        timer.start()\n",
    "        for X, y in train_iter:\n",
    "            # 为单个小批量执行多GPU训练\n",
    "            train_batch(X, y, device_params, devices, lr)\n",
    "            torch.cuda.synchronize() # 这个函数是一个同步的函数，就是保证每个GPU都跑完了\n",
    "        timer.stop()\n",
    "        # 在GPU0上评估模型\n",
    "        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(\n",
    "            lambda x: lenet(x, device_params[0]), test_iter, devices[0]),))\n",
    "    print(f'测试精度：{animator.Y[0][-1]:.2f}，{timer.avg():.1f}秒/轮，'\n",
    "          f'在{str(devices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd2963-26d3-45ac-9c86-33d6279263b4",
   "metadata": {},
   "source": [
    "上面我们实现了两个函数，分别是对单个batch和整个数据集上进行GPU数据并行训练，下面是一些解释：\n",
    "1. 首先，我们在每个GPU上面进行了损失函数的计算与梯度反向传播，注意此处我们只算了总的梯度，并没有算平均的损失（实际上我们是要算batch平均损失然后去算梯度的）\n",
    "2. 接着我们把所有的GPU上的梯度加起来，因为是导数运算规则相加是直接加，所以我们此时得到了总损失计算得到的梯度\n",
    "3. 我们再用自己实现的sgd优化器来根据梯度进行优化，这里我们的batch size一定要使用全尺寸的小批量，为啥呢？因为我们前面把多块GPU的损失加起来了算了梯度，等于这里我们得到的是总的batch的损失，自然要加上全尺寸大小去求平均值（可以看d2l.sgd的源代码）\n",
    "4. 对于一个epoch来说，其他的其实都差不多，我们首先用并行的方法来进行训练，然后在每个epoch的末尾，我们在其中一个GPU上用训练得到的参数进行一次评估就行（因为评估等于只做前向运算，所以不并行无所谓了）\n",
    "5. 其实会发现，我们上面算loss和算梯度都有一点点串行，并没有真正的并行。框架一般都会自动的帮你实现并行，所以我们不需要特地去写啥复杂的代码来并行（MXNET、TF都确定性，torch好像也是可以的）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaf77b7-e723-41c4-bfc5-3421a17fd85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(num_gpus=1, batch_size=256, lr=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a623a9-8814-4e74-8e19-1c6a8824f1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(num_gpus=2, batch_size=256, lr=0.2)\n",
    "# 其实实际情况下这里并没有变快，可能因为是LeNet本身小，然后batch size也不大，有很多种原因\n",
    "# 一般来说，GPU数量增大，每个GPU的batch size应该保持不变的\n",
    "# batch size变大了，其实lr也可以适当的变大一点，因为每次学习的样本变多了，等于一个epoch里面学习次数少了，所以lr适当大一点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a37777-2ef6-4959-b51a-497e33a9ecdb",
   "metadata": {},
   "source": [
    "接下来我们看看简洁实现是怎么做的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9321a4-698a-4afe-b5de-6ab1045af7e9",
   "metadata": {},
   "source": [
    "这里我们用个ResNet18，相对来说是个更正常大小的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4ccc7ef-fcb0-4230-914a-71e3778e61e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T17:08:08.383183Z",
     "iopub.status.busy": "2023-12-16T17:08:08.382477Z",
     "iopub.status.idle": "2023-12-16T17:08:12.377647Z",
     "shell.execute_reply": "2023-12-16T17:08:12.376593Z",
     "shell.execute_reply.started": "2023-12-16T17:08:08.383118Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7127577a-5e67-4d52-bec6-d428f1337899",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T17:08:13.496143Z",
     "iopub.status.busy": "2023-12-16T17:08:13.495198Z",
     "iopub.status.idle": "2023-12-16T17:08:13.518999Z",
     "shell.execute_reply": "2023-12-16T17:08:13.516984Z",
     "shell.execute_reply.started": "2023-12-16T17:08:13.496067Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def resnet18(num_classes, in_channels=1):\n",
    "    \"\"\"稍加修改的ResNet-18模型\"\"\"\n",
    "    def resnet_block(in_channels, out_channels, num_residuals,\n",
    "                     first_block=False):\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(d2l.Residual(in_channels, out_channels, use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.append(d2l.Residual(out_channels, out_channels))\n",
    "        return nn.Sequential(*blk)\n",
    "\n",
    "    # 该模型使用了更小的卷积核、步长和填充，而且删除了最大汇聚层\n",
    "    net = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU())\n",
    "    net.add_module(\"resnet_block1\", resnet_block(\n",
    "        64, 64, 2, first_block=True))\n",
    "    net.add_module(\"resnet_block2\", resnet_block(64, 128, 2))\n",
    "    net.add_module(\"resnet_block3\", resnet_block(128, 256, 2))\n",
    "    net.add_module(\"resnet_block4\", resnet_block(256, 512, 2))\n",
    "    net.add_module(\"global_avg_pool\", nn.AdaptiveAvgPool2d((1,1)))\n",
    "    net.add_module(\"fc\", nn.Sequential(nn.Flatten(),\n",
    "                                       nn.Linear(512, num_classes)))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e49d6a3d-d2be-4a37-ab63-a4ddf5b77dc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T17:08:43.944713Z",
     "iopub.status.busy": "2023-12-16T17:08:43.944209Z",
     "iopub.status.idle": "2023-12-16T17:08:44.086833Z",
     "shell.execute_reply": "2023-12-16T17:08:44.085804Z",
     "shell.execute_reply.started": "2023-12-16T17:08:43.944669Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = resnet18(10)\n",
    "# 获取GPU列表Residual类的实现有错误，所以改了源代码\n",
    "devices = d2l.try_all_gpus()\n",
    "# d2l包里面\n",
    "# 我们将在训练代码实现中初始化网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4a1f34e-44e4-4d95-84fa-622dbd9bd5c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T17:09:05.873802Z",
     "iopub.status.busy": "2023-12-16T17:09:05.872821Z",
     "iopub.status.idle": "2023-12-16T17:09:05.894881Z",
     "shell.execute_reply": "2023-12-16T17:09:05.893180Z",
     "shell.execute_reply.started": "2023-12-16T17:09:05.873729Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(net, num_gpus, batch_size, lr):\n",
    "    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "    devices = [d2l.try_gpu(i) for i in range(num_gpus)]\n",
    "    def init_weights(m):\n",
    "        if type(m) in [nn.Linear, nn.Conv2d]:\n",
    "            nn.init.normal_(m.weight, std=0.01)\n",
    "    net.apply(init_weights)\n",
    "    # 在多个GPU上设置模型\n",
    "    net = nn.DataParallel(net, device_ids=devices)\n",
    "    trainer = torch.optim.SGD(net.parameters(), lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    timer, num_epochs = d2l.Timer(), 10\n",
    "    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        timer.start()\n",
    "        for X, y in train_iter:\n",
    "            trainer.zero_grad()\n",
    "            X, y = X.to(devices[0]), y.to(devices[0])\n",
    "            l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "        timer.stop()\n",
    "        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(net, test_iter),))\n",
    "    print(f'测试精度：{animator.Y[0][-1]:.2f}，{timer.avg():.1f}秒/轮，'\n",
    "          f'在{str(devices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9a86a0-0c57-437f-9d24-40b81efb2eb5",
   "metadata": {},
   "source": [
    "这里的训练函数其实和上面的没啥本质区别，只是写的更加简洁了：\n",
    "1. 这里我们对net重新进行了一次实例化，调用nn.DataParallel类，完成并行\n",
    "2. 然后直接用torch的SGD优化器\n",
    "3. 对每个batch，首先把数据放在GPU上，然后直接像我们之前单GPU网络实现那样，算loss，反向，优化就OK了\n",
    "4. 总的来说torch的并行实现让我们保留了和单GPU一样的语法风格，这样很方便\n",
    "5. 这里我们的net是nn.module的子类，所以可以发现，我们不需要自己定义参数，初始化也可以很方便的用apply函数来进行\n",
    "6. 而上面我们用functional实现的LeNet，就需要自己定义参数，并且传进去，初始化也要自己来弄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd5c284-b991-4bc9-80fc-015e6e8ed12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(net, num_gpus=1, batch_size=256, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064c6096-6aee-4644-a72e-5657b2d8174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(net, num_gpus=2, batch_size=512, lr=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216291a6-cb5d-4eac-8429-31b113457282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f81e32-a569-4a14-bea8-0cfa3e7a984b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8797c6-9774-4cec-a118-50537254313c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaea001-028a-4cd4-8752-df3ab0093cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96f31ad-9af6-4441-838f-9c84e37ebb40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f633fb-d5f7-46bc-9499-2041cc6032b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73bc6a3-033b-4224-92e1-26cf4ebc3289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
