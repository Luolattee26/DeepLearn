{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece05231-0d50-4a97-95a4-05add4540ae7",
   "metadata": {},
   "source": [
    "AlexNet算是开启了持续到现在的深度学习风潮"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04342c8-9c14-4af4-b077-8c72128b81fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T20:43:19.897229Z",
     "iopub.status.busy": "2023-12-13T20:43:19.896342Z",
     "iopub.status.idle": "2023-12-13T20:43:19.920411Z",
     "shell.execute_reply": "2023-12-13T20:43:19.917541Z",
     "shell.execute_reply.started": "2023-12-13T20:43:19.897152Z"
    },
    "tags": []
   },
   "source": [
    "不过李沐老师说目前计算机视觉好像已经卷到头了？CNN可能已经被卷完了吧"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7009cd40",
   "metadata": {},
   "source": [
    "整个机器学习的潮流，大概是有三个阶段\n",
    "1. 神经网络\n",
    "2. 核方法：无敌的SVM\n",
    "3. 深度学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6567d79",
   "metadata": {},
   "source": [
    "核方法包括后面的随机森林啥的，有一个问题就是对特征提取很敏感，如果特征工程做的不好，那很容易就g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ab1663",
   "metadata": {},
   "source": [
    "相较于老前辈LeNet，AlexNet在架构上其实并没有非常本质的革新，但是在以下几点做出了改进：\n",
    "1. 用了dropout\n",
    "2. 加了relu（支持更深的模型，同时可以有更大的梯度，减缓梯度消失）\n",
    "3. 用了maxpooling（容易产生更大的梯度）\n",
    "4. 改变了计算机视觉领域（感觉是整个机器学习领域）的研究范式：\n",
    "    1. 以往是针对数据，进行人工特征提取，然后做SVM\n",
    "    2. 现在是用各种网络（比如conv layer），自动的学习特征（当然可以加上一点基于先验知识的人工特征提取），然后扔到softmax\n",
    "5. AlexNet做了一定的数据增强，不会把原始数据就扔进去"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8db8fb",
   "metadata": {},
   "source": [
    "看架构就可以发现，AlexNet充满一种立大砖飞的美感......"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce0225f",
   "metadata": {},
   "source": [
    "接下来看一下代码实现，这个就不在服务器上跑了，移步colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b17938a-eb2f-4875-9627-87109b0c4301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "net = nn.Sequential( # 因为整体架构还算简单，所以还是用Sequential\n",
    "    # 这里使用一个11*11的更大窗口来捕捉对象。\n",
    "    # 同时，步幅为4，以减少输出的高度和宽度。\n",
    "    # 另外，输出通道的数目远大于LeNet\n",
    "    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(), # 用的fashionMnist，所以通道1\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n",
    "    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    # 使用三个连续的卷积层和较小的卷积窗口。\n",
    "    # 除了最后的卷积层，输出通道的数量进一步增加。\n",
    "    # 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度\n",
    "    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nn.Flatten(),\n",
    "    # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合\n",
    "    nn.Linear(6400, 4096), nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(4096, 4096), nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    # 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000\n",
    "    nn.Linear(4096, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dd7650",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(1, 1, 224, 224)\n",
    "for layer in net:\n",
    "    X=layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t',X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75b2761",
   "metadata": {},
   "source": [
    "AlexNet是针对ImageNet来设计的，但是我们用的数据集是2828的，所以我们通过之前定义的函数拉大到224（没啥实际作用，只是演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2ade4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff715ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.01, 10\n",
    "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n",
    "# colab跑去\n",
    "# colab t4 gpu跑了10分钟......"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5146b3a6",
   "metadata": {},
   "source": [
    "李沐老师暴论：在transformer、注意力机制之前，其实整个深度学习没有什么本质上的创新，实际上cnn或者别的网络都是类似已经经验类型的创新，调着调着发现出来一个更好的设计，这样就出来了\n",
    "\n",
    "至于很多看起来牛逼的名词，实际上只是人为造出来的，比如relu，和各种deep"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
